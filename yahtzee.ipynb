{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a777fa1f-8cbf-4faf-91bc-5c4e7355bdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Device: NVIDIA RTX A4000\n",
      "Mon Jan 20 12:53:43 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A4000               On  |   00000000:83:00.0 Off |                  Off |\n",
      "| 41%   46C    P8             17W /  140W |       4MiB /  16376MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32774ae-b01c-43e3-9597-fe479f8924a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–Š         | 4106/50000 [43:29<11:00:42,  1.16it/s, Avg=43.3, Best=189.0, Temp=0.100]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dqn import YahtzeeAgent\n",
    "from encoder import StateEncoder\n",
    "from env import NUM_ACTIONS, ActionType, GameState, YahtzeeEnv\n",
    "from utils import plot_training_progress\n",
    "\n",
    "\n",
    "def evaluate_agent(agent: YahtzeeAgent, num_games: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate agent performance across multiple games.\n",
    "    Returns dict with detailed statistics.\n",
    "    \"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    encoder = StateEncoder()\n",
    "    scores = []\n",
    "\n",
    "    # Store original temperature/epsilon\n",
    "    if agent.use_boltzmann:\n",
    "        old_temp = agent.temperature\n",
    "        agent.temperature = 0.01  # Nearly deterministic\n",
    "    else:\n",
    "        old_eps = agent.epsilon\n",
    "        agent.epsilon = 0.01\n",
    "\n",
    "    # Run evaluation games\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_vec = encoder.encode(state)\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "\n",
    "            # Use greedy action selection\n",
    "            action_idx = agent.select_action_greedy(state_vec, valid_actions)\n",
    "            state, reward, done, _ = env.step(action_idx)\n",
    "            total_reward += reward\n",
    "\n",
    "        scores.append(total_reward)\n",
    "\n",
    "    # Restore exploration parameters\n",
    "    if agent.use_boltzmann:\n",
    "        agent.temperature = old_temp\n",
    "    else:\n",
    "        agent.epsilon = old_eps\n",
    "\n",
    "    # Calculate statistics\n",
    "    scores = np.array(scores)\n",
    "    stats = {\n",
    "        \"mean\": np.mean(scores),\n",
    "        \"median\": np.median(scores),\n",
    "        \"std\": np.std(scores),\n",
    "        \"min\": np.min(scores),\n",
    "        \"max\": np.max(scores),\n",
    "        \"scores\": scores,\n",
    "    }\n",
    "\n",
    "    # Print detailed results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Mean Score: {stats['mean']:.1f}\")\n",
    "    print(f\"Median Score: {stats['median']:.1f}\")\n",
    "    print(f\"Std Dev: {stats['std']:.1f}\")\n",
    "    print(f\"Min Score: {stats['min']:.1f}\")\n",
    "    print(f\"Max Score: {stats['max']:.1f}\")\n",
    "\n",
    "    # Plot score distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(scores, bins=20, edgecolor=\"black\")\n",
    "    plt.axvline(\n",
    "        stats[\"mean\"],\n",
    "        color=\"red\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=f\"Mean ({stats['mean']:.1f})\",\n",
    "    )\n",
    "    plt.axvline(\n",
    "        stats[\"median\"],\n",
    "        color=\"green\",\n",
    "        linestyle=\"dashed\",\n",
    "        label=f\"Median ({stats['median']:.1f})\",\n",
    "    )\n",
    "    plt.title(\"Score Distribution\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def quick_validation_training(\n",
    "    num_episodes: int = 5000,  # Reduced episodes for quick validation\n",
    "    batch_size: int = 1024,  # Larger batch size to improve GPU utilization\n",
    "    eval_games: int = 20,  # Fewer eval games\n",
    "    eval_interval: int = 500,  # More frequent evaluation\n",
    ") -> tuple:\n",
    "    \"\"\"Quick training loop for validating the approach.\"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    encoder = StateEncoder()\n",
    "\n",
    "    # Initialize agent with modified parameters\n",
    "    agent = YahtzeeAgent(\n",
    "        state_size=encoder.state_size,\n",
    "        action_size=NUM_ACTIONS,\n",
    "        batch_size=batch_size,\n",
    "        gamma=0.99,\n",
    "        learning_rate=2e-4,  # Slightly higher learning rate\n",
    "        target_update=250,  # More frequent target updates\n",
    "        use_boltzmann=True,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    # Training metrics\n",
    "    all_rewards = []\n",
    "    best_mean = 0\n",
    "    eval_stats = []\n",
    "\n",
    "    # Training loop with progress bar\n",
    "    progress = tqdm(range(num_episodes), desc=\"Validation Training\")\n",
    "    for episode in progress:\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_vec = encoder.encode(state)\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "\n",
    "            action_idx = agent.select_action(state_vec, valid_actions)\n",
    "            next_state, reward, done, _ = env.step(action_idx)\n",
    "            next_state_vec = encoder.encode(next_state)\n",
    "\n",
    "            # Train DQN\n",
    "            agent.train_step(\n",
    "                state_vec,\n",
    "                action_idx,\n",
    "                reward,\n",
    "                next_state_vec,\n",
    "                done,\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "\n",
    "        # Update progress more frequently\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_score = np.mean(all_rewards[-50:])\n",
    "            progress.set_postfix(\n",
    "                {\"Avg\": f\"{avg_score:.1f}\", \"Temp\": f\"{agent.temperature:.3f}\"}\n",
    "            )\n",
    "\n",
    "        # Evaluate periodically\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            print(f\"\\nQuick evaluation at episode {episode + 1}...\")\n",
    "            stats = evaluate_agent(agent, num_games=eval_games)\n",
    "            eval_stats.append(stats)\n",
    "\n",
    "            if stats[\"mean\"] > best_mean:\n",
    "                best_mean = stats[\"mean\"]\n",
    "                agent.save(\"quick_val_best.pth\")\n",
    "\n",
    "            plot_training_progress(\n",
    "                all_rewards,\n",
    "                window=50,\n",
    "                title=(\n",
    "                    f\"Validation Progress (Episode {episode + 1})\\n\"\n",
    "                    f\"Best Mean: {best_mean:.1f}\"\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    return agent, all_rewards, eval_stats\n",
    "\n",
    "\n",
    "def train_yahtzee_agent(\n",
    "    num_episodes: int = 50000,  # Reduced episodes\n",
    "    use_boltzmann: bool = True,\n",
    "    plot_interval: int = 1000,  # More frequent plotting\n",
    "    eval_interval: int = 5000,  # More frequent evaluation\n",
    ") -> tuple:\n",
    "    \"\"\"Train a Yahtzee agent and return the trained agent + reward history.\"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    encoder = StateEncoder()\n",
    "\n",
    "    # Instantiate agent with optimized parameters\n",
    "    agent = YahtzeeAgent(\n",
    "        state_size=encoder.state_size,\n",
    "        action_size=NUM_ACTIONS,\n",
    "        batch_size=512,  # Smaller batch size\n",
    "        gamma=0.99,  # Slightly lower discount\n",
    "        learning_rate=1e-4,  # Higher learning rate\n",
    "        target_update=500,  # More frequent updates\n",
    "        use_boltzmann=use_boltzmann,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    # Training metrics\n",
    "    all_rewards = []\n",
    "    best_score = 0\n",
    "    recent_scores = []\n",
    "    eval_stats = []\n",
    "    plateau_counter = 0\n",
    "    best_mean = 0\n",
    "\n",
    "    # Training loop with progress bar\n",
    "    progress = tqdm(range(num_episodes), desc=\"Training\")\n",
    "    for episode in progress:\n",
    "        state = env.reset()\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state_vec = encoder.encode(state)\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "\n",
    "            action_idx = agent.select_action(state_vec, valid_actions)\n",
    "            next_state, reward, done, _ = env.step(action_idx)\n",
    "            next_state_vec = encoder.encode(next_state)\n",
    "\n",
    "            # Train DQN\n",
    "            agent.train_step(\n",
    "                state_vec,\n",
    "                action_idx,\n",
    "                reward,\n",
    "                next_state_vec,\n",
    "                done,\n",
    "            )\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        all_rewards.append(total_reward)\n",
    "        recent_scores.append(total_reward)\n",
    "        if len(recent_scores) > 100:  # Shorter window\n",
    "            recent_scores.pop(0)\n",
    "\n",
    "        if total_reward > best_score:\n",
    "            best_score = total_reward\n",
    "\n",
    "        # Update progress bar more frequently\n",
    "        if (episode + 1) % 50 == 0:  # More frequent updates\n",
    "            avg_score = np.mean(recent_scores)\n",
    "            if use_boltzmann:\n",
    "                temp = agent.temperature\n",
    "                progress.set_postfix(\n",
    "                    {\n",
    "                        \"Avg\": f\"{avg_score:.1f}\",\n",
    "                        \"Best\": f\"{best_score:.1f}\",\n",
    "                        \"Temp\": f\"{temp:.3f}\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                eps = agent.epsilon\n",
    "                progress.set_postfix(\n",
    "                    {\n",
    "                        \"Avg\": f\"{avg_score:.1f}\",\n",
    "                        \"Best\": f\"{best_score:.1f}\",\n",
    "                        \"Eps\": f\"{eps:.3f}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # More frequent evaluation\n",
    "        if (episode + 1) % eval_interval == 0:\n",
    "            print(f\"\\nEvaluating at episode {episode + 1}...\")\n",
    "            stats = evaluate_agent(agent, num_games=50)  # Fewer eval games\n",
    "            eval_stats.append(stats)\n",
    "\n",
    "            if stats[\"mean\"] > best_mean:\n",
    "                best_mean = stats[\"mean\"]\n",
    "                plateau_counter = 0\n",
    "                # Save best model so far\n",
    "                agent.save(\"best_model.pth\")\n",
    "            else:\n",
    "                plateau_counter += 1\n",
    "\n",
    "            if plateau_counter >= 3:\n",
    "                print(\"\\nTraining has plateaued. Consider stopping.\")\n",
    "\n",
    "            plot_training_progress(\n",
    "                all_rewards,\n",
    "                window=100,  # Shorter window\n",
    "                title=f\"Training Progress (Episode {episode + 1})\\n\"\n",
    "                f\"Best Eval Mean: {best_mean:.1f}\",\n",
    "            )\n",
    "\n",
    "    return agent, all_rewards, eval_stats\n",
    "\n",
    "\n",
    "def simulate_game(agent: YahtzeeAgent, render: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Simulate a single game with visualization of dice and scoresheet.\n",
    "    Returns the final score.\n",
    "    \"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    encoder = StateEncoder()\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    turn = 1\n",
    "\n",
    "    # Store original temperature/epsilon\n",
    "    if agent.use_boltzmann:\n",
    "        old_temp = agent.temperature\n",
    "        agent.temperature = 0.01  # Nearly deterministic\n",
    "    else:\n",
    "        old_eps = agent.epsilon\n",
    "        agent.epsilon = 0.01\n",
    "\n",
    "    while not done:\n",
    "        if render:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"\\n=== Turn {turn} ===\")\n",
    "            print(env.render())\n",
    "\n",
    "        state_vec = encoder.encode(state)\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        # Get Q-values for all actions\n",
    "        q_values = agent.get_q_values(state_vec)\n",
    "        # Mask invalid actions\n",
    "        mask = np.full(agent.action_size, float(\"-inf\"))\n",
    "        mask[valid_actions] = 0\n",
    "        q_values = q_values + mask\n",
    "\n",
    "        # Select best action\n",
    "        action_idx = q_values.argmax()\n",
    "        action = env.IDX_TO_ACTION[action_idx]\n",
    "\n",
    "        if render:\n",
    "            print(\"\\nAgent's decision:\")\n",
    "            if action.kind == ActionType.ROLL:\n",
    "                print(\"Action: ROLL all dice\")\n",
    "            elif action.kind == ActionType.HOLD_MASK:\n",
    "                held = [i + 1 for i, hold in enumerate(action.data) if hold]\n",
    "                print(f\"Action: Hold dice at positions {held}\")\n",
    "            else:\n",
    "                print(f\"Action: Score {action.data.name}\")\n",
    "                print(f\"Expected value: {q_values[action_idx]:.1f}\")\n",
    "\n",
    "        state, reward, done, _ = env.step(action_idx)\n",
    "        total_reward += reward\n",
    "\n",
    "        if render:\n",
    "            if action.kind == ActionType.SCORE:\n",
    "                print(f\"Scored {reward:.1f} points\")\n",
    "                turn += 1\n",
    "            time.sleep(1)  # Pause to make it easier to follow\n",
    "\n",
    "    if render:\n",
    "        clear_output(wait=True)\n",
    "        print(\"\\n=== Game Over ===\")\n",
    "        print(env.render())\n",
    "        print(f\"\\nFinal Score: {total_reward:.1f}\")\n",
    "\n",
    "    # Restore exploration parameters\n",
    "    if agent.use_boltzmann:\n",
    "        agent.temperature = old_temp\n",
    "    else:\n",
    "        agent.epsilon = old_eps\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def show_action_values(\n",
    "    agent: YahtzeeAgent, state: Optional[GameState] = None, num_top: int = 5\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Show expected values for all valid actions in the current state.\n",
    "    If state is None, starts a new game.\n",
    "    Returns (state, valid_actions, q_values) for further use.\n",
    "    \"\"\"\n",
    "    env = YahtzeeEnv()\n",
    "    encoder = StateEncoder()\n",
    "\n",
    "    if state is None:\n",
    "        state = env.reset()\n",
    "\n",
    "    print(\"\\nCurrent Game State:\")\n",
    "    print(env.render())\n",
    "\n",
    "    # Get state encoding and valid actions\n",
    "    state_vec = encoder.encode(state)\n",
    "    valid_actions = env.get_valid_actions()\n",
    "\n",
    "    # Get Q-values and mask invalid actions\n",
    "    q_values = agent.get_q_values(state_vec)\n",
    "    mask = np.full(agent.action_size, float(\"-inf\"))\n",
    "    mask[valid_actions] = 0\n",
    "    q_values = q_values + mask\n",
    "\n",
    "    # Sort actions by Q-value\n",
    "    valid_q = [(i, q_values[i]) for i in valid_actions]\n",
    "    valid_q.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"\\nTop Actions and Their Expected Values:\")\n",
    "    for i, (action_idx, value) in enumerate(valid_q[:num_top]):\n",
    "        action = env.IDX_TO_ACTION[action_idx]\n",
    "        if action.kind == ActionType.ROLL:\n",
    "            print(f\"{i+1}. ROLL all dice (EV: {value:.1f})\")\n",
    "        elif action.kind == ActionType.HOLD_MASK:\n",
    "            held = [i + 1 for i, hold in enumerate(action.data) if hold]\n",
    "            if held:\n",
    "                print(f\"{i+1}. Hold dice {held} (EV: {value:.1f})\")\n",
    "            else:\n",
    "                print(f\"{i+1}. ROLL all dice (EV: {value:.1f})\")\n",
    "        else:\n",
    "            print(f\"{i+1}. Score {action.data.name} (EV: {value:.1f})\")\n",
    "\n",
    "    return state, valid_q[:num_top]\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Train with more episodes and evaluation:\n",
    "    agent, rewards, eval_stats = train_yahtzee_agent(\n",
    "        num_episodes=50000,\n",
    "        use_boltzmann=True,\n",
    "        plot_interval=1000,\n",
    "        eval_interval=5000,\n",
    "    )\n",
    "\n",
    "    # Final evaluation and save best model\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    stats = evaluate_agent(agent, num_games=100)\n",
    "    print(f\"\\nFinal mean score: {stats['mean']:.1f}\")\n",
    "    print(f\"Final median score: {stats['median']:.1f}\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = \"yahtzee_dqn_improved.pth\"\n",
    "    agent.save(model_path)\n",
    "    print(f\"Saved agent to {model_path}\")\n",
    "\n",
    "    # Interactive mode\n",
    "    env = YahtzeeEnv()  # For calculation mode\n",
    "    current_state = None\n",
    "\n",
    "    while True:\n",
    "        print(\"\\nChoose mode:\")\n",
    "        print(\"1. Simulation Mode (watch agent play)\")\n",
    "        print(\"2. Calculation Mode (see action values)\")\n",
    "        print(\"3. Exit\")\n",
    "\n",
    "        choice = input(\"\\nEnter choice (1-3): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            print(\"\\nSimulating a game...\")\n",
    "            simulate_game(agent, render=True)\n",
    "        elif choice == \"2\":\n",
    "            print(\"\\nStarting calculation mode...\")\n",
    "            current_state, valid_actions = show_action_values(agent)\n",
    "\n",
    "            while True:\n",
    "                print(\"\\nOptions:\")\n",
    "                print(\"1. Take an action and continue\")\n",
    "                print(\"2. Start new game\")\n",
    "                print(\"3. Return to main menu\")\n",
    "\n",
    "                subchoice = input(\"\\nEnter choice (1-3): \").strip()\n",
    "\n",
    "                if subchoice == \"1\":\n",
    "                    try:\n",
    "                        prompt = \"\\nEnter action number: \"\n",
    "                        action_num = int(input(prompt).strip())\n",
    "                        if 1 <= action_num <= len(valid_actions):\n",
    "                            action_idx = valid_actions[action_num - 1][0]\n",
    "                            next_state, reward, done, _ = env.step(action_idx)\n",
    "\n",
    "                            if done:\n",
    "                                msg = f\"\\nGame Over! Final Score: {reward:.1f}\"\n",
    "                                print(msg)\n",
    "                                break\n",
    "                            else:\n",
    "                                # Get next state values\n",
    "                                result = show_action_values(agent, current_state)\n",
    "                                current_state, valid_actions = result\n",
    "                        else:\n",
    "                            print(\"\\nInvalid action number!\")\n",
    "                    except ValueError:\n",
    "                        print(\"\\nPlease enter a valid number!\")\n",
    "                elif subchoice == \"2\":\n",
    "                    current_state, valid_actions = show_action_values(agent)\n",
    "                else:\n",
    "                    break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"\\nThanks for playing!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3c1b9-29e7-4a43-b556-47a88e5f3fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
